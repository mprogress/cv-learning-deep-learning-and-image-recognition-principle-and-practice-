{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "5.1神经网络"
    ]
   },
   "outputs": [],
   "source": [
    "#1.一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好\n",
    "\n",
    "#2.过拟合：网络对于数据中的噪声（数据）有过强的拟合能力，而没有重视数据之间的潜在的基本关系\n",
    "\n",
    "#3.如果数据不是特别复杂，则似乎小一点的神经网络更好，因为可以防止过拟合。\n",
    "   #然而防止神经网络的过拟合还有许多其它的方法：L2正则化、dropout和输入噪声等，使用这些方法比减少神经元数要更好。\n",
    "\n",
    "#4.激活函数：在所有的隐藏层之间添加一个合适的激活函数，从而可以输出一个非线性函数。（激活函数可以是线性也可以是非线性）\n",
    "    #因为数据经过激活函数的输出之后区间是有范围的，因此除了分类问题之外，一般情况下不会考虑在隐藏层与输出层之间使用激活函数。\n",
    "    #分类问题中，一般，类问题可以考虑使用Sigmoid函数；多分类问题，可以考虑使用Softmax的激活函数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#常用的激活函数\n",
    "\n",
    "\n",
    "#  Sigmoid函数（详见4.2）   因为其会造成梯度消失，因此已经不大使用\n",
    "#  在靠近0和1这两端的时候，因为曲线变得非常的平缓。因此梯度基本为0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Tanh函数（双曲正切函数）\n",
    "#  公式：y = [e^x - e^(-x)]/[e^x + e^(-x)]\n",
    "# Tanh函数在输入很大或者是很小的时候，梯度很小，不利于权重更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ReLU函数（线性整流函数/修正性线性单元）\n",
    "#  公式 ： y = max(0,x)\n",
    "#  优点：在随即梯度下降的训练当中收敛很快，在输入为正数时，不存在梯度饱和，只有线性关系，不管是前向传播还是反向传播都比Sigmoid函数要快得多\n",
    "\n",
    "#  实现代码：\n",
    "import numpy as np\n",
    "def _relu(x):\n",
    "    return  np.maximum(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### 5.1.3  前向传播 #############################\n",
    "\n",
    "#神经网络前向传递的过程关键步骤\n",
    "\n",
    "#1.输入层的每个节点，都需要与隐藏层的每个节点做点对点的计算，计算的方法是加权求和 + 激活函数\n",
    "#2.利用隐藏层计算出的每个值，再同样与输出层进行计算（针对简单神经网络，若是具有多个隐藏层的神经网络，则重复以上过程）\n",
    "#3.在隐藏层，将隐藏层通过计算得到的值，进行（选取）激活函数的激活\n",
    "#4.输入层的数值将通过网络计算分别传播到隐藏层，再以同样的方式传播到输出层，最终的输出值与样本值进行比较。计算出误差，这\n",
    "#    这个过程称为前向传播 \n",
    "\n",
    "\n",
    "#  最后我们比较发现预测值和真实值之间存在误差，所以要进行反向传播来使误差减小（不断优化迭代，更新权重）\n",
    "\n",
    "# 在点对点乘法的过程当中，借助矩阵实现复杂的神经网络的计算\n",
    "\n",
    "##（权重矩阵W）·（输入层数据）= （结果值）\n",
    "## 判断权重矩阵的形状   取决于链接的两个层之间的节点对应关系，（前一层节点个数，后一层节点个数）\n",
    "#  例如：第一层的W1的形状取决于输入层 ，设输入层值有2个，第一个隐藏层的节点有3个，则W1的形状为（2，3）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "5.2输出层"
    ]
   },
   "outputs": [],
   "source": [
    "# 5.2.1 Softmax  \n",
    "\n",
    "# 对于多分类问题，我们需要使用Softmax分类器，它的输出是每个类别的概率\n",
    "# Softmax函数的定义，在多分类（C>2）时：  Si = e^(Vi) /  ∑(e^(Vi))  \n",
    "#  其中Vi表示分类器前级输出单元的输出，i表示类别索引，总类别个数 C ，Si是当前元素的指数与所有元素指数和的比值。\n",
    "#代码：\n",
    "# x为输入的向量\n",
    "def _softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "\n",
    "# 当运算的值很大或者很小时容易发生上溢出或者下溢出，因此在实际应用中，我们需要对V进行一些数值处理\n",
    "#    D = max(V)    Si = e^(Vi-D) / ∑(e^(Vi-D))\n",
    "# 代码\n",
    "# x为输入的向量\n",
    "def _softmax(x):\n",
    "    c = np.max(x)\n",
    "    exp_x = np.exp(x-c)\n",
    "    return exp_x / np.sum(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hotencoding \n",
    "\n",
    "#  独热码，直观来说就是有多少个状态就有多少个比特。而且只有一个比特为1，其余全为0的一种码制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出层的神经元个数\n",
    "\n",
    "#输出层的神经元数量应根据实际需要解决的问题来决定。对于分类问题，输出层的神经元个数一般会与类别的数量保持一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 广播原则\n",
    "# 如果两个数组的后源维度（从末尾开始算起的维度）的轴长度相符，或者其中一方的长度为1，则认为他们是广播兼容的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "5.5损失函数"
    ]
   },
   "outputs": [],
   "source": [
    "# 损失函数  神经网络模型得意实现是经过前向传播计算loss，根据loss的值进行反向推导，并进行相关参数的调整。\n",
    "# 常用的损失函数主要有 均方误差和交叉熵误差。\n",
    "\n",
    "\n",
    "# 5.5.1 均方误差\n",
    "# 均方误差  是各项数据偏离真实值的距离平方和的平均数，也即误差平方和的平均数。\n",
    "# 公式：  loss = ∑（xi-xi'）^2 / n\n",
    "# 其中xi 表示的是神经网络的输出，xi'表示的是真实值，i代表每个数据\n",
    "#  代码\n",
    "def men_squared_error(p,y):\n",
    "    return np.sum((p-y)**2)/y.shape[0]\n",
    "\n",
    "\n",
    "# 5.5.2 交叉熵误差\n",
    "# 公式  ：   loss = -∑log(y_predict(j))\n",
    "# 代码 \n",
    "def cross_entropy_error(p,y):\n",
    "    delta = le-7     #为避免出现log0\n",
    "    return np.sum(-y*np.log(p))\n",
    "\n",
    "\n",
    "#当真实类别和神经网络给出的类别相同的时候，损失函数的输出比较小\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
